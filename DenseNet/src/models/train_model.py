"""
Script de entrenamiento para clasificaci√≥n de radiograf√≠as de t√≥rax usando DenseNet
con transfer learning.

"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
import os
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from tqdm import tqdm
import json
from datetime import datetime

class ChestXrayDataset(Dataset):
    """
    Dataset personalizado para radiograf√≠as de t√≥rax.
    
    Estructura esperada:
    data/
    ‚îú‚îÄ‚îÄ chest_xray/     # Im√°genes de radiograf√≠as de t√≥rax
    ‚îî‚îÄ‚îÄ other_images/   # Otras im√°genes (no radiograf√≠as)
    """
    
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.images = []
        self.labels = []
        
        # Mapeo de clases
        self.class_to_idx = {'chest_xray': 0, 'other_images': 1}
        self.idx_to_class = {0: 'chest_xray', 1: 'other_images'}
        
        # Cargar im√°genes y etiquetas
        self._load_data()
    
    def _load_data(self):
        """Cargar todas las im√°genes y sus etiquetas."""
        for class_name in self.class_to_idx.keys():
            class_dir = os.path.join(self.data_dir, class_name)
            if not os.path.exists(class_dir):
                print(f"Advertencia: Directorio {class_dir} no encontrado")
                continue
                
            for img_name in os.listdir(class_dir):
                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    img_path = os.path.join(class_dir, img_name)
                    self.images.append(img_path)
                    self.labels.append(self.class_to_idx[class_name])
        
        print(f"Total de im√°genes cargadas: {len(self.images)}")
        print(f"Radiograf√≠as de t√≥rax: {self.labels.count(0)}")
        print(f"Otras im√°genes: {self.labels.count(1)}")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]
        
        # Cargar imagen
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

class DenseNetClassifier(nn.Module):
    """
    Clasificador DenseNet con transfer learning y freeze del backbone.
    """
    
    def __init__(self, num_classes=2, pretrained=True, freeze_backbone=True):
        super(DenseNetClassifier, self).__init__()
        
        # Cargar DenseNet pre-entrenado
        self.backbone = models.densenet121(pretrained=pretrained)
        
        # Congelar par√°metros del backbone para evitar overfitting
        if freeze_backbone:
            self._freeze_backbone()
        
        # Obtener n√∫mero de caracter√≠sticas de la √∫ltima capa
        num_features = self.backbone.classifier.in_features
        
        # Reemplazar la capa clasificadora
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )
    
    def _freeze_backbone(self):
        """Congelar todos los par√°metros del backbone (features)."""
        for param in self.backbone.parameters():
            param.requires_grad = False
        print("üîí Backbone congelado - solo se entrenar√° el clasificador")
    
    def unfreeze_backbone(self):
        """Descongelar el backbone para fine-tuning opcional."""
        for param in self.backbone.parameters():
            param.requires_grad = True
        print("üîì Backbone descongelado - fine-tuning habilitado")
    
    def forward(self, x):
        return self.backbone(x)

def get_transforms():
    """Obtener transformaciones de datos para entrenamiento y validaci√≥n."""
    
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, val_transform

def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001):
    """
    Entrenar el modelo.
    
    Args:
        model: Modelo a entrenar
        train_loader: DataLoader para datos de entrenamiento
        val_loader: DataLoader para datos de validaci√≥n
        num_epochs: N√∫mero de √©pocas
        learning_rate: Tasa de aprendizaje
    """
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # Funci√≥n de p√©rdida y optimizador
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    
    # Historial de entrenamiento
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    best_val_acc = 0.0
    best_model_state = None
    
    print(f"Entrenando en dispositivo: {device}")
    
    # Mostrar informaci√≥n de par√°metros
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    print(f"Total de par√°metros: {total_params:,}")
    print(f"Par√°metros entrenables: {trainable_params:,}")
    print(f"Par√°metros congelados: {frozen_params:,}")
    print(f"Porcentaje entrenable: {trainable_params/total_params*100:.2f}%")
    
    for epoch in range(num_epochs):
        print(f"\n√âpoca {epoch+1}/{num_epochs}")
        print("-" * 50)
        
        # Entrenamiento
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc="Entrenando")
        for batch_idx, (data, target) in enumerate(train_pbar):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            train_total += target.size(0)
            train_correct += (predicted == target).sum().item()
            
            train_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*train_correct/train_total:.2f}%'
            })
        
        # Validaci√≥n
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc="Validando")
            for data, target in val_pbar:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()
                
                val_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*val_correct/val_total:.2f}%'
                })
        
        # Calcular m√©tricas
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
        
        print(f"P√©rdida de entrenamiento: {train_loss:.4f}")
        print(f"Precisi√≥n de entrenamiento: {train_acc:.2f}%")
        print(f"P√©rdida de validaci√≥n: {val_loss:.4f}")
        print(f"Precisi√≥n de validaci√≥n: {val_acc:.2f}%")
        
        # Guardar mejor modelo
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()
            print(f"¬°Nuevo mejor modelo! Precisi√≥n: {val_acc:.2f}%")
        
        scheduler.step()
    
    # Cargar mejor modelo
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nMejor precisi√≥n de validaci√≥n: {best_val_acc:.2f}%")
    
    return {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accuracies': train_accuracies,
        'val_accuracies': val_accuracies,
        'best_val_acc': best_val_acc
    }

def plot_training_history(history, save_path=None):
    """Visualizar el historial de entrenamiento."""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Gr√°fico de p√©rdidas
    ax1.plot(history['train_losses'], label='Entrenamiento', color='blue')
    ax1.plot(history['val_losses'], label='Validaci√≥n', color='red')
    ax1.set_title('P√©rdida durante el entrenamiento')
    ax1.set_xlabel('√âpoca')
    ax1.set_ylabel('P√©rdida')
    ax1.legend()
    ax1.grid(True)
    
    # Gr√°fico de precisiones
    ax2.plot(history['train_accuracies'], label='Entrenamiento', color='blue')
    ax2.plot(history['val_accuracies'], label='Validaci√≥n', color='red')
    ax2.set_title('Precisi√≥n durante el entrenamiento')
    ax2.set_xlabel('√âpoca')
    ax2.set_ylabel('Precisi√≥n (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Gr√°fico guardado en: {save_path}")
    
    plt.show()

def evaluate_model(model, test_loader, class_names):
    """Evaluar el modelo en el conjunto de prueba."""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in tqdm(test_loader, desc="Evaluando"):
            data, target = data.to(device), target.to(device)
            output = model(data)
            _, predicted = torch.max(output, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # Reporte de clasificaci√≥n
    print("\n" + "="*50)
    print("REPORTE DE CLASIFICACI√ìN")
    print("="*50)
    print(classification_report(all_targets, all_predictions, 
                              target_names=class_names))
    
    # Matriz de confusi√≥n
    cm = confusion_matrix(all_targets, all_predictions)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Matriz de Confusi√≥n')
    plt.ylabel('Etiqueta Real')
    plt.xlabel('Etiqueta Predicha')
    plt.show()
    
    return all_predictions, all_targets

def main():
    """Funci√≥n principal para entrenar el modelo."""
    
    # Configuraci√≥n optimizada para datasets peque√±os (~1200 im√°genes)
    DATA_DIR = "data"  # Directorio con las im√°genes
    BATCH_SIZE = 16    # Reducido para datasets peque√±os
    NUM_EPOCHS = 3    # Aumentado para compensar el freeze
    LEARNING_RATE = 0.001
    TRAIN_SPLIT = 0.7
    VAL_SPLIT = 0.15
    TEST_SPLIT = 0.15
    FREEZE_BACKBONE = True  # Congelar backbone para evitar overfitting
    FINE_TUNE_EPOCHS = 5    # √âpocas adicionales con fine-tuning (opcional)
    
    # Verificar que existe el directorio de datos
    if not os.path.exists(DATA_DIR):
        print(f"Error: Directorio {DATA_DIR} no encontrado.")
        print("Por favor, crea la siguiente estructura:")
        print("data/")
        print("‚îú‚îÄ‚îÄ chest_xray/     # Im√°genes de radiograf√≠as de t√≥rax")
        print("‚îî‚îÄ‚îÄ other_images/   # Otras im√°genes (no radiograf√≠as)")
        return
    
    # Obtener transformaciones
    train_transform, val_transform = get_transforms()
    
    # Crear dataset completo
    full_dataset = ChestXrayDataset(DATA_DIR, transform=None)
    
    if len(full_dataset) == 0:
        print("No se encontraron im√°genes en el directorio especificado.")
        return
    
    # Dividir dataset
    dataset_size = len(full_dataset)
    train_size = int(TRAIN_SPLIT * dataset_size)
    val_size = int(VAL_SPLIT * dataset_size)
    test_size = dataset_size - train_size - val_size
    
    train_indices = list(range(train_size))
    val_indices = list(range(train_size, train_size + val_size))
    test_indices = list(range(train_size + val_size, dataset_size))
    
    # Crear datasets divididos
    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)
    val_dataset = torch.utils.data.Subset(full_dataset, val_indices)
    test_dataset = torch.utils.data.Subset(full_dataset, test_indices)
    
    # Aplicar transformaciones
    train_dataset.dataset.transform = train_transform
    val_dataset.dataset.transform = val_transform
    test_dataset.dataset.transform = val_transform
    
    # Crear DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    
    print(f"Tama√±o del conjunto de entrenamiento: {len(train_dataset)}")
    print(f"Tama√±o del conjunto de validaci√≥n: {len(val_dataset)}")
    print(f"Tama√±o del conjunto de prueba: {len(test_dataset)}")
    
    # Crear modelo con backbone congelado
    model = DenseNetClassifier(num_classes=2, pretrained=True, freeze_backbone=FREEZE_BACKBONE)
    
    # Entrenar modelo con backbone congelado
    print("\nüèãÔ∏è  Iniciando entrenamiento con backbone congelado...")
    print("="*60)
    history = train_model(model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE)
    
    # Opcional: Fine-tuning con backbone descongelado
    if FINE_TUNE_EPOCHS > 0:
        print(f"\nüîì Iniciando fine-tuning con backbone descongelado...")
        print("="*60)
        print("‚ö†Ô∏è  Advertencia: Fine-tuning puede causar overfitting en datasets peque√±os")
        
        # Descongelar backbone
        model.unfreeze_backbone()
        
        # Reducir learning rate para fine-tuning
        fine_tune_lr = LEARNING_RATE * 0.1
        print(f"Learning rate reducido a: {fine_tune_lr}")
        
        # Entrenar con fine-tuning
        fine_tune_history = train_model(model, train_loader, val_loader, FINE_TUNE_EPOCHS, fine_tune_lr)
        
        # Combinar historiales
        history['train_losses'].extend(fine_tune_history['train_losses'])
        history['val_losses'].extend(fine_tune_history['val_losses'])
        history['train_accuracies'].extend(fine_tune_history['train_accuracies'])
        history['val_accuracies'].extend(fine_tune_history['val_accuracies'])
        history['best_val_acc'] = max(history['best_val_acc'], fine_tune_history['best_val_acc'])
    
    # Guardar modelo
    model_path = "densenet_chest_xray_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'class_to_idx': full_dataset.class_to_idx,
        'idx_to_class': full_dataset.idx_to_class,
        'history': history
    }, model_path)
    print(f"\nModelo guardado en: {model_path}")
    
    # Visualizar historial de entrenamiento
    plot_training_history(history, "training_history.png")
    
    # Evaluar en conjunto de prueba
    class_names = ['chest_xray', 'other_images']
    predictions, targets = evaluate_model(model, test_loader, class_names)
    
    # Guardar configuraci√≥n de entrenamiento
    config = {
        'batch_size': BATCH_SIZE,
        'num_epochs': NUM_EPOCHS,
        'learning_rate': LEARNING_RATE,
        'train_split': TRAIN_SPLIT,
        'val_split': VAL_SPLIT,
        'test_split': TEST_SPLIT,
        'freeze_backbone': FREEZE_BACKBONE,
        'fine_tune_epochs': FINE_TUNE_EPOCHS,
        'best_val_acc': history['best_val_acc'],
        'total_epochs': NUM_EPOCHS + FINE_TUNE_EPOCHS,
        'timestamp': datetime.now().isoformat()
    }
    
    with open('training_config.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\nConfiguraci√≥n guardada en: training_config.json")
    print("\n¬°Entrenamiento completado!")

if __name__ == "__main__":
    main()
